{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Visualize Feature Maps / Activations\n",
    "\n",
    "Visualize feature maps (activations) and filters of Mask R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = \"C:\\\\Users\\\\Martin\\\\Documents\\\\LEGOFinder\\\\Keras\\\\lego_object_detection\\\\maskrcnn\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Imports for keras-vis\n",
    "from keras import activations\n",
    "from vis.visualization import visualize_activation, get_num_filters\n",
    "from vis.input_modifiers import Jitter\n",
    "from vis.utils import utils\n",
    "\n",
    "from samples.lego import lego\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Lego trained weights\n",
    "#LEGO_WEIGHTS_PATH = \"C:/Users/Martin/Documents/LEGOFinder/Keras/lego_object_detection/maskrcnn/snapshots/mask_rcnn_lego_0025.h5\" # Referenz Gewichte mit guten Resultaten\n",
    "LEGO_WEIGHTS_PATH = \"C:/Users/Martin/Google Drive/Colab/maskrcnn/snapshots/lego20200605T0653/mask_rcnn_lego_0040.h5\"\n",
    "\n",
    "# load either \"train\", \"val\" or \"eval\"\n",
    "DATASET = \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = lego.LegoConfig()\n",
    "LEGO_DIR = os.path.join(ROOT_DIR, \"datasets\", \"lego\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.8\n",
    "    USE_RPN_ROIS = True\n",
    "    USE_RANDOM_RPN_ROIS = False\n",
    "    USE_STAGE_TWO = False\n",
    "    BACKBONE = \"resnet152\"\n",
    "    PLOT_GRAPH = True\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on. Useful if you're training a model on the same machine, in which case use CPU and leave the GPU for training.\n",
    "DEVICE = \"/cpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes values: 'inference' or 'training' TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "dataset = lego.LegoDataset()\n",
    "dataset.load_lego(LEGO_DIR, DATASET)\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# print model\n",
    "#model.keras_model.summary()\n",
    "\n",
    "# show final model graph\n",
    "#if config.PLOT_GRAPH:\n",
    "#    plot_model(model.keras_model, to_file='maskrcnn_graph.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to lego weights file\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", LEGO_WEIGHTS_PATH)\n",
    "model.load_weights(LEGO_WEIGHTS_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose image to analyse\n",
    "if DATASET == \"eval\":\n",
    "    image_id = dataset.get_image_id(\"0000000001.png\") # if eval set us the image with the Lego haufen\n",
    "else:\n",
    "    image_ids = np.random.choice(dataset.image_ids, 1)\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "image, image_meta, gt_class_ids, gt_bboxes, gt_masks = modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Feature Maps (Activations)\n",
    "\n",
    "In some cases it helps to look at the output from different layers and visualize them to catch issues and odd patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations of a few sample layers from top level mrcnn model\n",
    "if config.USE_RPN_ROIS:\n",
    "    # With keras-resnet as backbone\n",
    "    mrcnn_model_layer_names  = ['input_image', 'conv1', 'conv1_relu', 'res2c_relu', 'res3d_relu', 'res4f_relu', 'fpn_p6', 'fpn_p4', 'fpn_p3']\n",
    "\n",
    "    # If native Mask R-CNN is used as backbone\n",
    "    # activation_layer_names = ['input_image', 'conv1', 'activation_1', 'res2c_out', 'res3c_out', 'res4w_out', 'fpn_p6', 'fpn_p4']\n",
    "else:\n",
    "    mrcnn_model_layer_names  = ['input_image', 'conv1', 'conv1_relu', 'res2c_relu', 'res3d_relu', 'res4f_relu', 'fpn_p6', 'fpn_p4', 'input_norm_classifier']\n",
    "\n",
    "# Get layers of backbone\n",
    "outputs = [(layer, tf.identity(model.keras_model.get_layer(layer).output)) for layer in mrcnn_model_layer_names]\n",
    "\n",
    "if config.USE_RPN_ROIS:\n",
    "    activations = model.run_graph([image], outputs, config)\n",
    "else:\n",
    "    activations = model.run_graph([image], outputs, config, gt_class_ids, gt_bboxes, gt_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image (normalized)\n",
    "_ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0],config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show backbone feature map\n",
    "max_maps = 10\n",
    "\n",
    "for name in mrcnn_model_layer_names:\n",
    "    d = activations[name].shape[-1]\n",
    "    dd = d if d <= max_maps else max_maps\n",
    "    titles = [name + '_' + str(i) for i in range(dd)]\n",
    "    display_images(np.transpose(activations[name][0,:,:,:dd], [2, 0, 1]), cols=max_maps, titles=titles, figsize=20, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Convnet Filters\n",
    "\n",
    "Now we will visualize the main building block of a CNN, the filters. There is one catch though, we wonâ€™t actually visualize the filters themselves, but instead we will display the patterns each filter maximally responds to. Remember that the filters are of size 3x3 meaning they have the height and width of 3 pixels, pretty small. So as a proxy to visualizing a filter, we will generate an input image where this filter activates the most. We will visualize filters at the last layer of each convolution block. To clear any confusion, in the previous section we visualized the feature maps, the output of the convolution operation. Now we are visualizing the filters, the main structure used in the convolution operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize filters using custom function\n",
    "from mrcnn import visualize\n",
    "\n",
    "layer_name = 'conv1'\n",
    "layer_name = 'res3a_branch2a'\n",
    "stitched_filter_images = visualize.visualize_layer(model.keras_model, layer_name, filter_range=(0, None), grid_dimensions=None)\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "plt.title(layer_name)\n",
    "plt.axis('off')\n",
    "plt.imshow(stitched_filter_images)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do same for RPN model\n",
    "\n",
    "outputs = []\n",
    "\n",
    "# Find RPN model\n",
    "for layer in model.keras_model.layers:\n",
    "    # Is the layer a model?\n",
    "    if layer.__class__.__name__ == 'Model':\n",
    "        rpn_model = layer\n",
    "        print(\"Found RPN model.\")\n",
    "\n",
    "# Get layers of backbone\n",
    "outputs.append(('input_image', tf.identity(model.keras_model.get_layer('input_image').output)))\n",
    "outputs.append(('conv1', tf.identity(model.keras_model.get_layer('conv1').output)))\n",
    "outputs.append(('rpn_class_raw', tf.identity(rpn_model.get_layer('rpn_class_raw').output)))\n",
    "outputs.append(('rpn_bbox_pred', tf.identity(rpn_model.get_layer('rpn_bbox_pred').output)))\n",
    "\n",
    "print(outputs)\n",
    "activations = model.run_graph([image], outputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To debug and install keras-viz do:\n",
    "#\n",
    "# conda activate maskrcnn\n",
    "# cd C:\\Users\\Martin\\Documents\\LEGOFinder\\Software\\keras-vis-master>\n",
    "# python setup.py install --record files.txt\n",
    "\n",
    "layer_idx = utils.find_layer_idx(model.keras_model, 'conv1')\n",
    "\n",
    "img = visualize_activation(model.keras_model, layer_idx, filter_indices=10, tv_weight=0., input_modifiers=[Jitter(0.05)], max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import activations\n",
    "from vis.visualization import visualize_activation, get_num_filters\n",
    "from vis.input_modifiers import Jitter\n",
    "from vis.utils import utils\n",
    "\n",
    "\n",
    "max_filters = 2\n",
    "selected_indices = []\n",
    "vis_images = [[], [], [], [], []]\n",
    "i = 0\n",
    "\n",
    "# Specify certain filter index if you want manually\n",
    "selected_filters = [[0, 3, 11, 25, 26, 33, 42, 62], \n",
    "                    [8, 21, 23, 38, 39, 45, 50, 79], \n",
    "                    [40, 48, 52, 54, 81, 107, 224, 226],\n",
    "                    [58, 79, 86, 216, 307, 426, 497, 509],\n",
    "                    [2, 7, 41, 84, 103, 306, 461, 487]]\n",
    "\n",
    "selected_filters = 0\n",
    "\n",
    "conv_layer_names        = ['conv1']\n",
    "\n",
    "for layer_name in conv_layer_names:\n",
    "    layer_idx = utils.find_layer_idx(model.keras_model, layer_name)\n",
    "\n",
    "    # Visualize all filters in this layer.\n",
    "    if selected_filters:\n",
    "        filter_indices = selected_filters[i]\n",
    " \n",
    "    else:\n",
    "        # Select random filters / kernels, but not more than max_filters\n",
    "        n = get_num_filters(model.keras_model.layers[layer_idx])\n",
    "        filter_indices = sorted(np.random.permutation(n)[:max_filters])\n",
    "\n",
    "    selected_indices.append(filter_indices)\n",
    "\n",
    "    print(\"Layer {} has {} fitlers, filter {} is selected.\".format(layer_name, n, filter_indices))\n",
    "\n",
    "    # Generate input image for each filter.\n",
    "    for idx in filter_indices:\n",
    "        img = visualize_activation(model.keras_model, layer_idx, filter_indices=idx, tv_weight=0., input_modifiers=[Jitter(0.05)], max_iter=300) \n",
    "        vis_images[i].append(img)\n",
    "\n",
    "\n",
    "    # Generate stitched image palette with 4 cols so we get 2 rows.\n",
    "    stitched = utils.stitch_images(vis_images[i], cols=4)    \n",
    "    plt.figure(figsize=(20, 30))\n",
    "    plt.title(layer_name)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(stitched)\n",
    "    plt.show()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vis_images = [[], [], [], [], []]\n",
    "i = 0\n",
    "for layer_name in conv_layer_names:\n",
    "    layer_idx = utils.find_layer_idx(model.keras_model, layer_name)\n",
    "\n",
    "    # Generate input image for each filter.\n",
    "    for j, idx in enumerate(selected_indices[i]):\n",
    "        img = visualize_activation(model.keras_model, layer_idx, filter_indices=idx, seed_input=vis_images[i][j], input_modifiers=[Jitter(0.05)]) \n",
    "        img = utils.draw_text(img, 'Filter {}'.format(idx))  \n",
    "        new_vis_images[i].append(img)\n",
    "\n",
    "    stitched = utils.stitch_images(new_vis_images[i], cols=4)    \n",
    "    plt.figure(figsize=(20, 30))\n",
    "    plt.title(layer_name)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(stitched)\n",
    "    plt.show()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.5 64-bit ('maskrcnn': conda)",
   "language": "python",
   "name": "python35564bitmaskrcnnconda12328bed15fa469aad72f1a08ed0d096"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}