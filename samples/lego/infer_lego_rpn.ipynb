{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask R-CNN - Inspect RPN Model\n",
    "\n",
    "Code and visualizations to test, debug, and evaluate the RPN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = \"C:\\\\Users\\\\Martin\\\\Documents\\\\LEGOFinder\\\\Keras\\\\lego_object_detection\\\\maskrcnn\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "from samples.lego import lego\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Lego trained weights\n",
    "#LEGO_WEIGHTS_PATH = \"C:/Users/Martin/Documents/LEGOFinder/Keras/lego_object_detection/maskrcnn/snapshots/mask_rcnn_lego_0025.h5\" # Referenz Gewichte mit guten Resultaten\n",
    "#LEGO_WEIGHTS_PATH = \"C:/Users/Martin/Google Drive/Colab/maskrcnn/snapshots/lego20200614T1246/mask_rcnn_lego_0020.h5\"\n",
    "LEGO_WEIGHTS_PATH = \"C:/Users/Martin/Documents/LEGOFinder/Keras/lego_object_detection/maskrcnn/snapshots/lego20200620T1156/mask_rcnn_lego_0040.h5\"\n",
    "\n",
    "# load either \"train\", \"val\" or \"eval\"\n",
    "DATASET = \"eval\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = lego.LegoConfig()\n",
    "LEGO_DIR = os.path.join(ROOT_DIR, \"datasets\", \"lego\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.8\n",
    "    USE_LRPN = False\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "dataset = lego.LegoDataset()\n",
    "dataset.load_lego(LEGO_DIR, DATASET)\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to lego weights file\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", LEGO_WEIGHTS_PATH)\n",
    "model.load_weights(LEGO_WEIGHTS_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 1: Region Proposal Network\n",
    "\n",
    "The Region Proposal Network (RPN) runs a lightweight binary classifier on a lot of boxes (anchors) over the image and returns object/no-object scores. Anchors with high *objectness* score (positive anchors) are passed to the stage two to be classified.\n",
    "\n",
    "Often, even positive anchors don't cover objects fully. So the RPN also regresses a refinement (a delta in location and size) to be applied to the anchors to shift it and resize it a bit to the correct boundaries of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose image to analyse\n",
    "if DATASET == \"eval\":\n",
    "    image_id = dataset.get_image_id(\"0000000002.png\") # if eval set us the image with the Lego haufen\n",
    "else:\n",
    "    image_ids = np.random.choice(dataset.image_ids, 1)\n",
    "    image_id = image_ids[0]\n",
    "    \n",
    "image, image_meta, gt_class_ids, gt_bboxes, gt_masks = modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "\n",
    "# select some GT boxes\n",
    "idx = [4, 8]\n",
    "\n",
    "#gt_bboxes = np.array(gt_bboxes[idx])\n",
    "#gt_class_ids = np.array(gt_class_ids[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.a RPN Targets\n",
    "\n",
    "The RPN targets are the training values for the RPN. To generate the targets, we start with a grid of anchors that cover the full image at different scales, and then we compute the IoU of the anchors with ground truth object. Positive anchors are those that have an IoU >= 0.7 with any ground truth object, and negative anchors are those that don't cover any object by more than 0.3 IoU. Anchors in between (i.e. cover an object by IoU >= 0.3 but < 0.7) are considered neutral and excluded from training.\n",
    "\n",
    "To train the RPN regressor, we also compute the shift and resizing needed to make the anchor cover the ground truth object completely.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RPN trainig targets (input for RPN)\n",
    "if config.USE_RPN_ROIS:\n",
    "\n",
    "    # Call get_anchors() to load anchors\n",
    "    molded_images, image_metas, windows = model.mold_inputs([image])\n",
    "    image_shape = molded_images[0].shape\n",
    "    model.get_anchors(image_shape)\n",
    "\n",
    "    target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets( image.shape, model.anchors, gt_class_ids, gt_bboxes, model.config)\n",
    "\n",
    "    log(\"target_rpn_match\", target_rpn_match)       # input_rpn_match -> fÃ¼r jeden Anchor ob dieser ein     1 = positive anchor (IoU > 0.7), \n",
    "                                                    #                                                      -1 = negative anchor (IoU < 0.3), \n",
    "                                                    #                                                       0 = neutral is im Vergleich zu den GT boxen\n",
    "    log(\"target_rpn_bbox\", target_rpn_bbox)         # input_rpn_bbox ->  Anchor bbox deltas against the GT boxes. This is the delta the RPN shout predict.\n",
    "\n",
    "    positive_anchor_ix = np.where(target_rpn_match[:] == 1)[0]\n",
    "    negative_anchor_ix = np.where(target_rpn_match[:] == -1)[0]\n",
    "    neutral_anchor_ix = np.where(target_rpn_match[:] == 0)[0]\n",
    "    positive_anchors = model.anchors[positive_anchor_ix]\n",
    "    negative_anchors = model.anchors[negative_anchor_ix]\n",
    "    neutral_anchors = model.anchors[neutral_anchor_ix]\n",
    "    log(\"positive_anchors\", positive_anchors)\n",
    "    log(\"negative_anchors\", negative_anchors)\n",
    "    log(\"neutral anchors\", neutral_anchors)\n",
    "\n",
    "    # Apply refinement deltas to positive anchors\n",
    "    refined_anchors = utils.apply_box_deltas(\n",
    "        positive_anchors,\n",
    "        target_rpn_bbox[:positive_anchors.shape[0]] * model.config.RPN_BBOX_STD_DEV)\n",
    "    log(\"refined_anchors\", refined_anchors, )\n",
    "\n",
    "    # Print dimensions of positive anchors\n",
    "    print(\"\\nBest matching anchors are (Image Dimensions={}):\\n\".format(image.shape))\n",
    "    for i, rect in enumerate(positive_anchors):\n",
    "        y1, x1, y2, x2 = rect\n",
    "        \n",
    "        print(\"Anchor {}: \\t\\t [{:.0f} {:.0f} {:.0f} {:.0f}] \\tw={:.0f} \\th={:.0f}\".format(i, x1, y1, x2, y2, x2-x1, y2-y1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACHTUNG DAS IST DER IDEAL FALL !!! \n",
    "#\n",
    "# Das Bild unten zeigt nur die refinded Boxen basieren auf GT den Boxen, nicht die vom RPN vorgeschlagenen (Proposals), siehe \n",
    "# nÃ¤chste Sektion unten. Es ist der Best Case, wenn das RPN perfekt funktionieren wÃ¼rde.\n",
    "#\n",
    "\n",
    "if config.USE_RPN_ROIS:\n",
    "    visualize.draw_boxes(image, boxes=positive_anchors, refined_boxes=refined_anchors, ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.b RPN Predictions\n",
    "\n",
    "Here we run the RPN graph and display its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RPN sub-graph\n",
    "if config.USE_RPN_ROIS:\n",
    "    pillar = model.keras_model.get_layer(\"ROI\").output\n",
    "\n",
    "    # TF 1.4 and 1.9 introduce new versions of NMS. Search for all names to support TF 1.3~1.10\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression:0\")\n",
    "    if nms_node is None:\n",
    "        nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV2:0\")\n",
    "    if nms_node is None: #TF 1.9-1.10\n",
    "        nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV3:0\")\n",
    "\n",
    "    rpn = model.run_graph([image], [\n",
    "        (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),         # Letzer Layer im RPN mit den propsosed Klassen Wahrscheinlichkeit GB oder FG pro Anker\n",
    "        (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI/pre_nms_anchors:0\")),\n",
    "        (\"refined_anchors\", model.ancestor(pillar, \"ROI/refined_anchors:0\")),\n",
    "        (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI/refined_anchors_clipped:0\")),\n",
    "        (\"post_nms_anchor_ix\", nms_node),                                       # Filtered anchors that have a NMS of RPN_NMS_THRESHOLD (>0.7)\n",
    "        (\"proposals\", model.keras_model.get_layer(\"ROI\").output),               # \"ROI\" ist name des ProposalLayers(), also die gefilterten rois proposals\n",
    "    ], config)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.c Analyse RPN Proposal Accuracy\n",
    "\n",
    "The idea is here to check how well the proposals match the GT. Hence for every lego (GT box) we check what is the maximum IoU. Then we simply form the average to get a single value for its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the proposal that has the highest IoU on each GT box and take average of all\n",
    "max_proposals_bbox = np.empty((gt_bboxes.shape[0], 4))\n",
    "max_proposals_iou = np.empty((gt_bboxes.shape[0], 1))\n",
    "\n",
    "# Get proposals from graph\n",
    "h, w = config.IMAGE_SHAPE[:2]\n",
    "proposals = rpn['proposals'][0, :] * np.array([h, w, h, w])\n",
    "\n",
    "print(\"Best proposals are:\\n\")\n",
    "for i, box in enumerate(gt_bboxes):\n",
    "\n",
    "    overlaps = utils.compute_overlaps(np.array([box.tolist()]), proposals)\n",
    "    iou_max = np.max(overlaps, axis=1)\n",
    "    iou_argmax = np.argmax(overlaps, axis=1)\n",
    "    max_proposals_bbox[i] = proposals[iou_argmax]\n",
    "    max_proposals_iou[i] = iou_max\n",
    "\n",
    "    y1, x1, y2, x2 = proposals[iou_argmax][0]\n",
    "    iou = iou_max[0]\n",
    "\n",
    "    print(\"Proposal Bbox {}: \\t[{:.0f} {:.0f} {:.0f} {:.0f}] w={:.0f} h={:.0f} iou={:.2f}\".format(i, x1, y1, x2, y2, x2-x1, y2-y1, iou))\n",
    "\n",
    "\n",
    "print(\"\\nAverage maximum IoU is: {0:0.2f}\".format(np.average(max_proposals_iou)))\n",
    "visualize.draw_boxes(image, boxes=gt_bboxes, refined_boxes=max_proposals_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.d Analyse RPN Anchors for GT Boxes in Detail\n",
    "\n",
    "Here we try to find the anchors with the highest probability for a certain GT. The question is if the RPN has predicted the right anchors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.USE_RPN_ROIS:\n",
    "\n",
    "    # Get anchor probabilties, second column with FG probabilites of an anchor  \n",
    "    anchors_prob_fg = rpn['rpn_class'][:,:,1].flatten()\n",
    "\n",
    "    # For each GT box, find anchors which overlap at least a little bit and get the propbability that the RPN has predicted for this anchor\n",
    "    for i, box in enumerate(gt_bboxes):\n",
    "\n",
    "        print(\"\\n--> Analysing GT box {}\\n\".format(i))\n",
    "        matched_anchors = []\n",
    "        overlaps = utils.compute_overlaps(model.anchors, np.array([box.tolist()]))\n",
    "\n",
    "        # Get anchor indices where IoU > x\n",
    "        anchor_idxs = np.where(overlaps >= 0.2)[0]\n",
    "\n",
    "        for i,idx in enumerate(anchor_idxs):\n",
    "\n",
    "            # use only anchors with a minimum probability only\n",
    "            if anchors_prob_fg[idx] >= 0.9:\n",
    "\n",
    "                print(\"    Anchor {:.0f} IoU={:0.2f}, probability={:0.4f}\".format(idx, overlaps[idx][0], anchors_prob_fg[idx]))\n",
    "                matched_anchors.append(model.anchors[idx])\n",
    "\n",
    "        if len(matched_anchors) == 0:\n",
    "            print(\"Ups - no anchor found with this IoU and probability. Adjust values.\")\n",
    "        else:\n",
    "            visualize.draw_boxes(image, boxes=np.asarray(matched_anchors), ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anchors by score (before refinement) -> here every lego should have at least one anchor, otherwise something is wrong.\n",
    "if config.USE_RPN_ROIS:\n",
    "    limit = 100\n",
    "    # Take second column with FG probabilites\n",
    "    anchors_prob_fg = rpn['rpn_class'][:,:,1].flatten()\n",
    "    sorted_anchor_ids = np.argsort(anchors_prob_fg)[::-1]\n",
    "    print(\"Top 100 anchor probabilties, max. {0:0.2f}, min. {1:0.2f}\".format(\n",
    "            anchors_prob_fg[sorted_anchor_ids[0]], anchors_prob_fg[sorted_anchor_ids[limit-1]]))\n",
    "    visualize.draw_boxes(image, boxes=model.anchors[sorted_anchor_ids[:limit]], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top anchors with refinement. Then with clipping to image boundaries\n",
    "if config.USE_RPN_ROIS:\n",
    "    limit = 200\n",
    "    ax = get_ax(1, 2)\n",
    "    pre_nms_anchors = utils.denorm_boxes(rpn[\"pre_nms_anchors\"][0], image.shape[:2])\n",
    "    refined_anchors = utils.denorm_boxes(rpn[\"refined_anchors\"][0], image.shape[:2])\n",
    "    refined_anchors_clipped = utils.denorm_boxes(rpn[\"refined_anchors_clipped\"][0], image.shape[:2]) # vor den NMS\n",
    "    visualize.draw_boxes(image,     boxes           = pre_nms_anchors[:limit],\n",
    "                                    refined_boxes   = refined_anchors[:limit], ax=ax[0])\n",
    "    visualize.draw_boxes(image,     refined_boxes   = refined_anchors_clipped[:limit], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show refined anchors after non-max suppression. -> Here some anchors from previous pictures might fall away that have overlays with other, to give place for new ones. This is a bit confusing to look at, since on the previous image, we might have seen less anchors.\n",
    "if config.USE_RPN_ROIS:\n",
    "    limit = 50\n",
    "    ixs = rpn[\"post_nms_anchor_ix\"][:limit]\n",
    "    visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[ixs], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final proposals.\n",
    "if config.USE_RPN_ROIS:\n",
    "    limit = 20\n",
    "    # Convert back to image coordinates for display\n",
    "    h, w = config.IMAGE_SHAPE[:2]\n",
    "    proposals = rpn['proposals'][0, :limit] * np.array([h, w, h, w])\n",
    "    visualize.draw_boxes(image, refined_boxes=proposals, ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MW: Final rois proposals from RPN network. Idealerweise sollten diese sehr Ã¤hnlich den GT Boxen sein."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.5 64-bit ('maskrcnn': conda)",
   "language": "python",
   "name": "python35564bitmaskrcnnconda12328bed15fa469aad72f1a08ed0d096"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}